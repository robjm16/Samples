{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e363e3ba7466f021f441b10bae41d4dc889125c"
   },
   "source": [
    "## Introduction   \n",
    "This notebook generates stock price predictions for a machine learning competition sponsored by Kaggle and Two Sigma, a hedge fund that uses artificial intelligence in its trading strategies.  The aim of the competition was to use financial and news data to predict how roughly 1,800 stocks will perform relative to their benchmarks 10 days out into the future.  \n",
    "\n",
    "Selected financial and news data dating back to 2008 was supplied by the sponsors.   Participants were restricted to using this data; no additional data could be imported. \n",
    "\n",
    "Daily predictions for each asset were made by assigning a confidence value (between 0 and 1) indicating whether a stock price was likely to rise.  A prediction of 0.99 indicated very strong certainty that a stock would rise in value 10 days out relative to its benchmark.  Similarly, a prediction of 0.01 indicated near certainty that the stock would underperform its benchmark, and a 0.50 prediction meant equal odds of rising or falling.  \n",
    "\n",
    "Final scores were calculated by summing the products of the confidence values times the actual relative returns for each stock for each day studied, and then dividing by the standard deviation of the daily scores.  In this way, the scoring was similar to the Sharpe ratio, a widely used measure of risk-adjusted financial returns.  \n",
    "\n",
    "In the first phase of the contest, prediction models were scored based solely on historical data.  In the second phase of the contest, to take place in early 2019, the models will be re-scored against future outcomes. \n",
    "\n",
    "To ensure a level playing field, all models had to be run online on Kaggle kernels that were subject to memory and time restrictions. \n",
    "\n",
    "Complete information about the contest, including the data sources, can be found  at:   \n",
    "\n",
    "https://www.kaggle.com/c/two-sigma-financial-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93d99328e3e0a661a940d83ae386f32e01c1f5d6"
   },
   "source": [
    "## My Approach \n",
    "Using the supplied data, I generated a variety of features, including rolling averages of financial returns and news story sentiment.  I used a ightGBM  model (a gradient boosted decision tree) as a baseline prediction model and to generate information on feature importances.  I reduced the features further using principal component analyses.  In the end, I found that a neural network model outperformed the lightGBM model as well as various ensemble models.  Accordingly, I used the neural network for final predictions.  Further details are covered below. \n",
    "\n",
    "At the end of the first phase of the competition, I ranked in the top 10% of nearly 2,900 teams/participants.  Final results will be based on prediction success against actual stock returns in early 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50bc29d151fa6192be326cbc343ce0b6e40405b4"
   },
   "source": [
    " ### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from kaggle.competitions import twosigmanews\n",
    "import time \n",
    "import gc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a60d768ec157469fe0512b86356dace3c41233a"
   },
   "outputs": [],
   "source": [
    "# Data available only via this Kernel-only environment \n",
    "env = twosigmanews.make_env()\n",
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a12977e00b94b1fb7b475aa5be21e9d9fbf956cf"
   },
   "outputs": [],
   "source": [
    "# Translate time values to datetime objects for easier filtering \n",
    "market_train_df.time=pd.to_datetime(market_train_df['time'], utc=True)\n",
    "news_train_df.time=pd.to_datetime(news_train_df['time'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "787b2dae2a4ff8a659ac1df7eb5066f9172f7c69"
   },
   "outputs": [],
   "source": [
    "# Reduce the number of observations used, due to time/memory constraints \n",
    "sample = True\n",
    "if sample:\n",
    "    market_train_df = market_train_df.loc[market_train_df.time >= '2009-1-1 00:00:00']\n",
    "    news_train_df = news_train_df.loc[news_train_df.time >= '2009-1-1 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15ca95df9e6416bf8074e6cafbba383f867f7fc7"
   },
   "outputs": [],
   "source": [
    "print(market_train_df.shape)\n",
    "print(news_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a706adec1ef336c19580226471a0bc004be10e58"
   },
   "outputs": [],
   "source": [
    "# Seed dataframes to be used later in final predictions; \n",
    "# provides lagging rolling averages leading up to the initial prediction days \n",
    "dates=market_train_df.time.unique()\n",
    "cutoff=dates[-20] # Identfies the last n days' cut-off date\n",
    "market_sliding_df = market_train_df.loc[market_train_df.time >= cutoff].copy().reset_index(drop=True)\n",
    "news_sliding_df = news_train_df.loc[news_train_df.time >= cutoff].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1dc18894b762f8ced842c6d597a5191eaf77cfc0"
   },
   "outputs": [],
   "source": [
    "del dates, cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9377b23a2091fd92dd2dced1a97536d6c9fcea9d"
   },
   "source": [
    " ### Prep Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6eb4f8ebdd0f38bae3053cd5803e17613a341a58"
   },
   "source": [
    "Many different data combinations/transformations were attempted, including different time windows and various moving average convergence/divergence analyses;  in the end, due to time/memory contstraints imposed by the contest sponsors, and  after testing feature importances on a lightGBM model, the following features were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2dd059945b22b154d0f5e2936bf4534aee29e4a"
   },
   "outputs": [],
   "source": [
    "def prep_market_data(mkt_trn_df, obs=False):\n",
    "    '''Processes the market data'''\n",
    "    # Trim the datetime object to just the date\n",
    "    mkt_trn_df.time = mkt_trn_df.time.dt.date\n",
    "\n",
    "    # Split the asset code into two parts -- main identifier and the suffix\n",
    "    mkt_trn_df['assetCodeID'], mkt_trn_df['assetCodeclass'] = mkt_trn_df[\n",
    "        'assetCode'].str.split('.', 1).str\n",
    "\n",
    "    # Fill NaNs for selected columns; the market_obs df does not have a 'returnsOpenNextMktres10' (target) column, thus the\n",
    "    # use of the \"obs\" boolean\n",
    "    if obs:\n",
    "        fill_cols = [\n",
    "            'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "            'returnsClosePrevMktres10', 'returnsOpenPrevMktres10'\n",
    "        ]\n",
    "    else:\n",
    "        fill_cols = [\n",
    "            'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "            'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n",
    "            'returnsOpenNextMktres10'\n",
    "        ]\n",
    "    mkt_trn_df[fill_cols] = mkt_trn_df[fill_cols].fillna(0)\n",
    "\n",
    "    # Get performance sign (positive or negative against benchmark) of backward-looking 1 and 10 day adjusted trend\n",
    "    mkt_trn_df['returnsOpenPrevMktres1Sign'] = np.sign(\n",
    "        mkt_trn_df.returnsOpenPrevMktres1)\n",
    "    mkt_trn_df['returnsOpenPrevMktres10Sign'] = np.sign(\n",
    "        mkt_trn_df.returnsOpenPrevMktres10)\n",
    "\n",
    "    # Add rolling stats, basic and exponential; due to memory constraints, using just one time window with final model\n",
    "    windows = [15]\n",
    "    for i in range(len(windows)):\n",
    "        mkt_trn_df[\n",
    "            'rollingOpenAdjustedMean_' + str(windows[i])] = mkt_trn_df.groupby(\n",
    "                \"assetCode\"\n",
    "            )['returnsOpenPrevMktres1'].apply(\n",
    "                lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        mkt_trn_df['rollingOpenAdjustedMedian_' + str(\n",
    "            windows[i]\n",
    "        )] = mkt_trn_df.groupby(\"assetCode\")['returnsOpenPrevMktres1'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).median())\n",
    "        mkt_trn_df['rollingOpenAdjustedSTD_' + str(\n",
    "            windows[i]\n",
    "        )] = mkt_trn_df.groupby(\"assetCode\")['returnsOpenPrevMktres1'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).std(ddof=0))\n",
    "        mkt_trn_df['rollingCloseAdjustedMean_' + str(\n",
    "            windows[i]\n",
    "        )] = mkt_trn_df.groupby(\"assetCode\")['returnsClosePrevMktres1'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        mkt_trn_df['rollingCloseAdjustedMedian_' + str(\n",
    "            windows[i]\n",
    "        )] = mkt_trn_df.groupby(\"assetCode\")['returnsClosePrevMktres1'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).median())\n",
    "        mkt_trn_df['rollingCloseAdjustedSTD_' + str(\n",
    "            windows[i]\n",
    "        )] = mkt_trn_df.groupby(\"assetCode\")['returnsClosePrevMktres1'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).std(ddof=0))\n",
    "        mkt_trn_df['rollingOpenPrevMktres1Sign_' + str(\n",
    "            windows[i])] = mkt_trn_df.groupby(\n",
    "                \"assetCode\"\n",
    "            )['returnsOpenPrevMktres1Sign'].apply(\n",
    "                lambda x: x.rolling(window=windows[i], min_periods=1).sum())\n",
    "        mkt_trn_df['rollingOpenPrevMktres10Sign_' + str(\n",
    "            windows[i])] = mkt_trn_df.groupby(\n",
    "                \"assetCode\"\n",
    "            )['returnsOpenPrevMktres10Sign'].apply(\n",
    "                lambda x: x.rolling(window=windows[i], min_periods=1).sum())\n",
    "        # exponentially weighted moving averages\n",
    "        mkt_trn_df['xrollingOpenAdjustedMean_' + str(\n",
    "            windows[i])] = mkt_trn_df.groupby(\n",
    "                \"assetCode\")['returnsOpenPrevMktres1'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        mkt_trn_df['xrollingCloseAdjustedMean_' + str(\n",
    "            windows[i])] = mkt_trn_df.groupby(\n",
    "                \"assetCode\")['returnsClosePrevMktres1'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "    # Rolling vs daily avg trading volume by asset\n",
    "    grouped = mkt_trn_df.groupby(['assetCode'])['volume'].apply(\n",
    "        lambda x: x.rolling(window=20, min_periods=1).mean()).to_frame()\n",
    "    grouped.rename(columns={'volume': 'meanRolling20Volume'}, inplace=True)\n",
    "    mkt_trn_df['meanRolling20Volume'] = grouped['meanRolling20Volume']\n",
    "    mkt_trn_df[\n",
    "        'dailyVolVSRollingAvg'] = mkt_trn_df.volume / mkt_trn_df.meanRolling20Volume\n",
    "    # Filling small number of observations that had a zero divisor with 1\n",
    "    mkt_trn_df.dailyVolVSRollingAvg.fillna(1, inplace=True)\n",
    "\n",
    "    return mkt_trn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dc02017b60b50513837f392bff7662b728cd2f2"
   },
   "outputs": [],
   "source": [
    "# Prep the market data \n",
    "market_train_df=prep_market_data(market_train_df)\n",
    "market_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d0ebec95c020eaa7c6165be5b28a00249564101"
   },
   "outputs": [],
   "source": [
    "market_train_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7982c80044fa73ed4337b79590ec5b33a40d80f1"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f693ce389e2afa819e299b13c96986c6bdd889c6"
   },
   "source": [
    "### Prep news data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5dcf999772ac462eaf5168b0366c7d612eaaa250"
   },
   "source": [
    "Features derived from news data had a far smaller impact on model performance than the financial data.  Other contest participants echoed this finding in online discussions.  After experimentation with many different feature options, the most important features aligned with the findings of a 2015 paper entitled, [\"Novel and Topical Business News and Their Impact on Stock Market Activity.\"](http://https://link.springer.com/article/10.1140/epjds/s13688-017-0123-7)    This paper found that only novel and unanticipated news had a meaningful impact on stock price performance.  My features likewise tended to focus on \"flash\" news stories, rather than general company commentary.  The only keywords that I found meaningful in terms of model performance were \"upgrade\" or \"downgrade.\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2590e203afd5714548d5f9fbbdc7c161a5f93a9b"
   },
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def get_assetCodeID(row):\n",
    "    '''For each row, removes the parentheses and extra quotation marks, and returns the first asset code'''\n",
    "    return row.replace(\"{\", '').replace(\"}\", '').replace(\"'\", '').split(\n",
    "        \".\", 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2590e203afd5714548d5f9fbbdc7c161a5f93a9b"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def prep_news_data(news_trn_df):\n",
    "    '''Processes the news data'''\n",
    "    # Trim the datetime object to just the date\n",
    "    date_cols = ['time']\n",
    "    for col in date_cols:\n",
    "        news_trn_df[col] = news_trn_df[col].dt.date\n",
    "\n",
    "    # Select only Reuters stories; breaking alerts (not general stories and articles); and only where company\n",
    "    # is in first sentence (ie, the clear subject)\n",
    "    news_trn_df = news_trn_df.loc[\n",
    "        (news_trn_df.provider == 'RTRS') & (news_trn_df.urgency <= 2)\n",
    "        & (news_trn_df.firstMentionSentence == 1)].copy()\n",
    "\n",
    "    # Split the asset code\n",
    "    news_trn_df['assetCodeID'] = news_trn_df['assetCodes'].apply(\n",
    "        get_assetCodeID)\n",
    "\n",
    "    # Series of steps to identify key topics (e.g., upgrades and downgrades)\n",
    "    # Group versions of articles that were actually updates of one article\n",
    "    # Create a label (category) encoder object\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    # Fit the encoder to the pandas column\n",
    "    le.fit(news_trn_df.sourceId)\n",
    "    # Apply the fitted encoder to the pandas column\n",
    "    news_trn_df.sourceId = le.transform(news_trn_df.sourceId)\n",
    "    #  Group updated articles as one\n",
    "    news_trn_df = news_trn_df.groupby('sourceId', as_index=False).tail(1)\n",
    "    # Identify key topics\n",
    "    keywords = [['UPGRADE'], ['DOWNGRADE']]\n",
    "    col = 'headline'\n",
    "\n",
    "    # Put within broader function to be able to use local variables\n",
    "    def topic_check(row):\n",
    "        \"\"\"Assigns result to topic column; 1 for yes (keyword appears in answer), 0 for no\"\"\"\n",
    "        if any([word in row[col] for word in keywords[i]]):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Loop through the list of keywords\n",
    "    for i in range(len(keywords)):\n",
    "        news_trn_df['headline_' + keywords[i][0]] = news_trn_df.apply(\n",
    "            topic_check, axis=1)\n",
    "\n",
    "    # Remove unused columns\n",
    "    droplist = [\n",
    "        'sourceTimestamp', 'firstCreated', 'sourceId', 'headline',\n",
    "        'takeSequence', 'assetName', 'assetCodes', 'wordCount',\n",
    "        'sentimentWordCount'\n",
    "    ]\n",
    "    news_trn_df.drop(droplist, axis=1, inplace=True)\n",
    "\n",
    "    # Group news by assetCodeID and date and generate summary stats\n",
    "    grouped = news_trn_df.groupby(['time', 'assetCodeID'])\n",
    "    # Dictionary with aggregations\n",
    "    d = {\n",
    "        'urgency':\n",
    "        ['count'],  # will use the count for article count (see further below)\n",
    "        'sentimentClass': ['mean', 'sum'],\n",
    "        'sentimentNegative': ['mean', 'sum'],\n",
    "        'sentimentNeutral': ['mean', 'sum'],\n",
    "        'sentimentPositive': ['mean', 'sum'],\n",
    "        'noveltyCount24H': ['mean', 'sum'],\n",
    "        'noveltyCount7D': ['mean', 'sum'],\n",
    "        'volumeCounts24H': ['mean', 'sum'],\n",
    "        'volumeCounts7D': ['mean', 'sum'],\n",
    "        'headline_UPGRADE': ['sum'],\n",
    "        'headline_DOWNGRADE': ['sum']\n",
    "    }\n",
    "    rev = grouped.agg(d).reset_index()\n",
    "    rev.columns = ['_'.join(col) for col in rev.columns.values]\n",
    "    rev.rename(\n",
    "        columns={\n",
    "            \"urgency_count\": 'articleCountDay',\n",
    "            'time_': 'time',\n",
    "            'assetCodeID_': 'assetCodeID'\n",
    "        },\n",
    "        inplace=True)  # check column name for assetCodeID\n",
    "\n",
    "    return rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5cc8a5e53068c90b1e99e0ab82d4cdd8e606391"
   },
   "outputs": [],
   "source": [
    "# Prep news data \n",
    "news_train_df=prep_news_data(news_train_df)\n",
    "news_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b8feb383b3632c055aa39c1b7daa63a718b2080"
   },
   "source": [
    "### Join Market and News Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c79c9740c56d035768436049e79298c354413449"
   },
   "outputs": [],
   "source": [
    "def prep_combined(mkt_trn_df, news_trn_df):\n",
    "    '''Combines the market and news data and creates new combined data features'''\n",
    "    # Merge the dataframes\n",
    "    combined_df = mkt_trn_df.merge(\n",
    "        news_trn_df, on=['time', 'assetCodeID'], how='left')\n",
    "\n",
    "    # Perform rolling average/exp average calculations\n",
    "    windows = [15]\n",
    "    for i in range(len(windows)):\n",
    "        combined_df['rollingArticleCountDayMean_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['articleCountDay'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingsentimentClass_sum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['sentimentClass_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingsentimentNegative_sum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['sentimentNegative_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingsentimentPositive_sum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['sentimentPositive_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingnoveltyCount24H_sum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['noveltyCount24H_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingnoveltyCount7D_sum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['noveltyCount7D_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingvolumeCounts24H_sum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['volumeCounts24H_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingvolumeCounts7D_sum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['volumeCounts7D_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingheadlineUpgrade_meanofsum_' + str(\n",
    "            windows[i]\n",
    "        )] = combined_df.groupby(\"assetCodeID\")['headline_UPGRADE_sum'].apply(\n",
    "            lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        combined_df['rollingheadlineDowngrade_meanofsum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\"\n",
    "            )['headline_DOWNGRADE_sum'].apply(\n",
    "                lambda x: x.rolling(window=windows[i], min_periods=1).mean())\n",
    "        # exponential rolling average\n",
    "        combined_df['xrollingArticleCountDayMean_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['articleCountDay'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingsentimentClass_sum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['sentimentClass_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingsentimentNegative_sum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['sentimentNegative_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingsentimentPositive_sum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['sentimentPositive_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingnoveltyCount24H_sum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['noveltyCount24H_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingnoveltyCount7D_sum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['noveltyCount7D_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingvolumeCounts24H_sum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['volumeCounts24H_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingvolumeCounts7D_sum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['volumeCounts7D_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingheadlineUpgrade_meanofsum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['headline_UPGRADE_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "        combined_df['xrollingheadlineDowngrade_meanofsum_' + str(\n",
    "            windows[i])] = combined_df.groupby(\n",
    "                \"assetCodeID\")['headline_DOWNGRADE_sum'].apply(\n",
    "                    lambda x: x.ewm(span=windows[i], min_periods=1).mean())\n",
    "\n",
    "    # Fill NaNs\n",
    "    non_cat_cols = [i for i in combined_df.columns if i != 'assetName']\n",
    "    for col in non_cat_cols:\n",
    "        combined_df[col].fillna(value=0, inplace=True)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9da039d51e14e4fb95eb122a6098ea71e16754f9"
   },
   "outputs": [],
   "source": [
    "# Join news and market data  \n",
    "combined_df=prep_combined(market_train_df, news_train_df)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b949d5282cc413c1edc675131dcd60c97ae9e5e"
   },
   "outputs": [],
   "source": [
    "combined_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09ed3dc40b1d710e73ce5c6d00c5fd8742959f9d"
   },
   "outputs": [],
   "source": [
    " del news_train_df, market_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32ceee9ff6f9ff57e3fa1db1ff22039e6933af40"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e203be981b83f95373b73cfc3236c4aba92f3f73"
   },
   "source": [
    "### Split into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7de8dd3bb586483337cd7546bbf205509f116eea"
   },
   "outputs": [],
   "source": [
    "# For classifying if stock went up or not(T/F)\n",
    "up = combined_df.returnsOpenNextMktres10 >= 0\n",
    "# List of features\n",
    "fcol = [\n",
    "    c for c in combined_df.columns\n",
    "    if c not in [\n",
    "        'time', 'assetCode', 'assetName', 'assetCodeID', 'assetCodeclass',\n",
    "        'volume', 'close', 'open', 'universe', 'meanRolling20Volume',\n",
    "        'returnsOpenNextMktres10', 'provider', 'urgency'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b4fdb9483601b44b56423d1363f80445afde333"
   },
   "outputs": [],
   "source": [
    "X = combined_df[fcol].values\n",
    "up = up.values  # True of False\n",
    "r = combined_df.returnsOpenNextMktres10.values  # Actual return values, not just up/down\n",
    "# Check \n",
    "assert X.shape[0] == up.shape[0] == r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fef96548386d7c3ee0cb41b3544a6bd39a66b7c"
   },
   "outputs": [],
   "source": [
    "# Scale the X values to 0-1\n",
    "mins = np.min(X, axis=0)\n",
    "maxs = np.max(X, axis=0)\n",
    "rng = maxs - mins\n",
    "X = 1 - ((maxs - X) / rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f505f2c549e6cb4be75c2c28eae9d59e77785e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Use to select the optimal number of pc's\n",
    "pca = PCA(n_components=35)\n",
    "pca.fit(X)\n",
    "# The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "print(var)\n",
    "# Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print (var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a2aab4d6e707a6deaf2391a1085524dcbcae515"
   },
   "outputs": [],
   "source": [
    "# Create the principal components\n",
    "N_COMP=15  # Use number as determined above \n",
    "pca = PCA(n_components=N_COMP)\n",
    "pca.fit(X)\n",
    "X1=pca.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1c41d6561f9e241e81f509de39cc18a782c5a85"
   },
   "outputs": [],
   "source": [
    "# Split rain/test to avoid any data leakage related to the availability of the leading 10-day target variable;\n",
    "# separate datasets by date, and leave a gap between train and test\n",
    "test_slice = combined_df.loc[combined_df.time >= dt.date(\n",
    "    year=2016, month=7, day=1)].shape[0]  # Last n mos of data\n",
    "dropout_slice = combined_df.loc[combined_df.time >= dt.date(\n",
    "    year=2016, month=6,\n",
    "    day=1)].shape[0] - test_slice  # One month drop-out interval\n",
    "train_slice = X1.shape[0] - test_slice - dropout_slice  # Everything up to dropout interval\n",
    "X_train = X1[:train_slice]\n",
    "X_test = X1[-test_slice:]\n",
    "up_train = up[:train_slice]\n",
    "up_test = up[-test_slice:]\n",
    "r_train = r[:train_slice]\n",
    "r_test = r[-test_slice:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43d29ae276961f4df5d4cf3ec65d19cfd43a51bd"
   },
   "outputs": [],
   "source": [
    "del combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "643b541bf4a887c9d1e5b685897d85cc6e918c34"
   },
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5c17c4b54f3bbd3dd7b17590c465404a9fb8cb7"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn.metrics import mean_absolute_error, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a84fc22fb90010f7411455f80fc88cbebe2edb06"
   },
   "outputs": [],
   "source": [
    "Xs=X_train\n",
    "ys=up_train.astype(int) # Translate T/F into 1/0\n",
    "Xs_val=X_test\n",
    "ys_val=up_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "153fab8c97bdf003b4a165d650a5caf24c3f80df"
   },
   "outputs": [],
   "source": [
    "del X_test, X_train, r, r_test, r_train, up, up_train, up_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47389433f3a57417c919348765ffc244699a55f6"
   },
   "outputs": [],
   "source": [
    "# Use for optimizing the neural network\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "callbacks_list = [\n",
    "    EarlyStopping(\n",
    "        patience=10, verbose=True, monitor='val_mean_absolute_error'),\n",
    "    ModelCheckpoint('model.hdf5', verbose=True, save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db2ef84e69de1dc040391c70de0cda8955dfcae1"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''Builds a keras/neural network model'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(N_COMP, input_dim=Xs.shape[1],\n",
    "                    activation='relu')) \n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['mae']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09bf23120ab80424113c79cfd7b381a092d50597"
   },
   "outputs": [],
   "source": [
    "## Train model\n",
    "seed = 7\n",
    "BATCH_SIZE = 128  # 32 is the default\n",
    "EPOCHS = 200\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = build_model()\n",
    "history = model.fit(\n",
    "    Xs,\n",
    "    ys,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(Xs_val, ys_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9caa501e2a6845680d959a8256d24aaf7697285e"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation log loss \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training log loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation log loss')\n",
    "plt.title('Training and validation log loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11505ec3a8f52c1ac0bb8f89d802ad7c3874c135"
   },
   "outputs": [],
   "source": [
    "# Retrieve train MAE (mean absolute error) and test MAE\n",
    "mae = history_dict['mean_absolute_error']\n",
    "val_mae = history_dict['val_mean_absolute_error']\n",
    "pd.options.display.max_rows = 999\n",
    "val_mae=[round(elem, 4) for elem in val_mae]\n",
    "val_mae_df=pd.DataFrame(val_mae)\n",
    "val_mae_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "132a56788a69d8bd8785e19e723cf3403fda63f2"
   },
   "outputs": [],
   "source": [
    "# Plot train MAE and test MAE\n",
    "plt.clf()\n",
    "plt.plot(epochs, mae, 'bo', label='Training MAE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MAE')\n",
    "plt.title('Training and validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10e75b14e65fa9c29e264b6dfa161634e91ed76e"
   },
   "outputs": [],
   "source": [
    "# Plot log loss vs test MAE\n",
    "plt.plot(range(1, len(loss_values) + 1), val_mae)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc578d918ec9ec00c3c1f19138899f0ab030e73a"
   },
   "outputs": [],
   "source": [
    "# Plot smoothed test MAE points \n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor  + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "# Excludes first n epochs from plot, eg [:10]\n",
    "smooth_mae_history = smooth_curve(val_mae)\n",
    "\n",
    "plt.plot(smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Smoothed Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c44ac48ed07364b84485a2a4fe51b0bc6b4c912"
   },
   "outputs": [],
   "source": [
    "# Create dataframe with smoothed points \n",
    "smval_mae=[round(elem, 4) for elem in smooth_mae_history]\n",
    "smval_mae_df=pd.DataFrame(smval_mae)\n",
    "smval_mae_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f980550d0cbca63dbb9bde2dc7f7521eb7deecf9"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f9d9f42687413225e647805b4f9ffdc770ea1c0"
   },
   "source": [
    "The model below proved best after experimentation with deeper/shallower networks and with regularization via dropout and l1/l2 regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe1ee8c1b149a8dfbe0c6a74f8676ec05b185412"
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "BATCH_SIZE = 128  # 32 is the default\n",
    "EPOCHS = 85\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = build_model()\n",
    "model.fit(Xs, ys, epochs=EPOCHS, batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a857a974731f3ef0deb483ccaba9782d74ddb2c8"
   },
   "source": [
    "  ### Make and Submit Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "857cfac6bd8016ed87370bbb90cc765e14486470"
   },
   "outputs": [],
   "source": [
    "# Delete columns not used in market_obs df and add a column for identifying which rows will be used for prediction\n",
    "market_sliding_df.drop(columns=['universe','returnsOpenNextMktres10'], inplace=True)\n",
    "market_sliding_df['makePrediction']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "917f1aa356e0f69ee90042af321dd2fb14ad8ffd"
   },
   "outputs": [],
   "source": [
    "# Use for limiting the size of the sliding prediction dfs(due to memory constraints)\n",
    "market_sliding_dflimit=market_sliding_df.shape[0] \n",
    "news_sliding_dflimit=news_sliding_df.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3755f1d824c34ad9ca3486f97df63ed0d5f7df14"
   },
   "outputs": [],
   "source": [
    "# Make and submit prediction within Kaggle environment\n",
    "days = env.get_prediction_days()\n",
    "n_days = 0\n",
    "\n",
    "# Loop through the generator object, which delivers a day's worth of market and news data, plus a prediction template\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    n_days += 1\n",
    "    print(\"Processing day: \", n_days)\n",
    "\n",
    "    # Pre-prep the data\n",
    "    market_obs_df.time = pd.to_datetime(market_obs_df['time'], utc=True)\n",
    "    market_obs_df['makePrediction'] = 1\n",
    "    market_sliding_df = market_sliding_df.append(\n",
    "        market_obs_df, ignore_index=True, sort=True)\n",
    "    market_sliding_df_copy = market_sliding_df.copy()\n",
    "\n",
    "    news_obs_df.time = pd.to_datetime(news_obs_df['time'])\n",
    "    news_sliding_df = news_sliding_df.append(\n",
    "        news_obs_df, ignore_index=True, sort=True)\n",
    "    news_sliding_df_copy = news_sliding_df.copy()\n",
    "\n",
    "    # Prep and combine data\n",
    "    market_sliding_prepped_df = prep_market_data(\n",
    "        market_sliding_df_copy,\n",
    "        obs=True)  # Need to use prep without the target value\n",
    "    news_sliding_prepped_df = prep_news_data(news_sliding_df_copy)\n",
    "    combined_sliding_df = prep_combined(market_sliding_prepped_df,\n",
    "                                        news_sliding_prepped_df)\n",
    "\n",
    "    # Extract the current day's rows for prediction\n",
    "    combined_sliding_df = combined_sliding_df.loc[\n",
    "        combined_sliding_df.makePrediction == 1].copy()\n",
    "\n",
    "    # Use only assetCodes in the prediction template\n",
    "    combined_obs_df = combined_sliding_df[combined_sliding_df.assetCode.isin(\n",
    "        predictions_template_df.assetCode)].copy()\n",
    "\n",
    "    # Limits sliding dfs to 2x the original size,for memory purposes, and preps sliding dfs for next round\n",
    "    market_sliding_df = market_sliding_df.tail(market_sliding_dflimit)\n",
    "    news_sliding_df = news_sliding_df.tail(news_sliding_dflimit)\n",
    "    market_sliding_df.makePrediction.replace(\n",
    "        to_replace=1, value=0, inplace=True)\n",
    "\n",
    "    # X data for each day\n",
    "    X_live = combined_obs_df[fcol].values\n",
    "    # Scale X data (based on earlier range values)\n",
    "    X_live = 1 - ((maxs - X_live) / rng)\n",
    "    # PCA-transformed\n",
    "    X_live = pca.transform(X_live)\n",
    "\n",
    "    # Make predictions\n",
    "    lp = model.predict(X_live).flatten()\n",
    "\n",
    "    # assign the probabilities to a confidence variable\n",
    "    confidence = lp\n",
    "    # Normalize confidence from -1 to 1\n",
    "    confidence = (confidence - confidence.min()) / (\n",
    "        confidence.max() - confidence.min())\n",
    "    confidence = confidence * 2 - 1\n",
    "    # Put data into the predictions df\n",
    "    preds = pd.DataFrame({\n",
    "        'assetCode': combined_obs_df['assetCode'],\n",
    "        'confidence': confidence\n",
    "    })\n",
    "    predictions_template_df = predictions_template_df.merge(\n",
    "        preds, how='left').drop(\n",
    "            'confidenceValue',\n",
    "            axis=1).fillna(0).rename(columns={\n",
    "                'confidence': 'confidenceValue'\n",
    "            })\n",
    "\n",
    "    # Adjustments to predictions\n",
    "    # Mean center (as benchmarks center to 0 each day)\n",
    "    predictions_template_df.confidenceValue = predictions_template_df.confidenceValue - predictions_template_df.confidenceValue.mean(\n",
    "    )\n",
    "    # Clip values in case mean centerring shifted values above/below 1 or -1\n",
    "    predictions_template_df.confidenceValue.clip(-1.0, 1.0, inplace=True)\n",
    "\n",
    "    env.predict(predictions_template_df)\n",
    "\n",
    "env.write_submission_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {
    "height": "304px",
    "left": "1555.92px",
    "right": "20px",
    "top": "163px",
    "width": "343.063px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
